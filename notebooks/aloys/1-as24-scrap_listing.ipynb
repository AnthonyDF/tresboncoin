{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "final-danger",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import re\n",
    "from PIL import Image \n",
    "import os \n",
    "import PIL \n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reserved-cherry",
   "metadata": {},
   "source": [
    "Bureau d'étude pour une fonction plus condensée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "collectible-strike",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cette phrase est écrite jeudi 10 à 10h du matin\n",
    "def scrap_pages():\n",
    "    #Start time\n",
    "    start_time = datetime.now()\n",
    "    datetime_1 = start_time.strftime(\"%Y-%m-%d_%Hh%M\")\n",
    "\n",
    "    #site to scrap\n",
    "    source = 'as24'\n",
    "\n",
    "    #log update\n",
    "    log_import = pd.read_csv('log.csv')\n",
    "    log_new = pd.DataFrame({'source' : [source],\n",
    "                            'step' : ['scrap pages'],\n",
    "                            'status' : ['started'],\n",
    "                            'time' : [datetime.now()],\n",
    "                            'details' : [\"\"]})\n",
    "    log = log_import.append(log_new , ignore_index=True)\n",
    "    log.to_csv('log.csv', index=False)\n",
    "\n",
    "    #fonction récurrente : extraction du nombre\n",
    "    def extract_int(sample):\n",
    "        list = re.findall(r'\\d+', sample)\n",
    "        while True:\n",
    "            try:\n",
    "                res = int(\"\".join(map(str, list)))\n",
    "                break\n",
    "            except ValueError:\n",
    "                res = np.nan\n",
    "                break\n",
    "        return res\n",
    "\n",
    "    #identifier le nombre de résultats et de pages d'une url listing d'annonces\n",
    "    def url_nb_results(url):\n",
    "        resp_single = requests.get(url)\n",
    "        soup_ = BeautifulSoup(resp_single.content, 'html.parser')\n",
    "        nb_offres = soup_.find('div', class_='cl-filters-summary__results-label sc-font-l sc-hidden-m sc-hidden-s sc-grid-row').find('span').text\n",
    "        return extract_int(nb_offres)\n",
    "    \n",
    "    def url_nb_pages(url):\n",
    "        nb_offres = url_nb_results(url)\n",
    "        if nb_offres%20>0: reste = 1\n",
    "        else: reste = 0\n",
    "        nb_pages = (nb_offres//20) + reste\n",
    "        return nb_pages\n",
    "\n",
    "    #Création du DataFrame avec tous les url de listing\n",
    "    #============================================================================\n",
    "    #=== pattern pour trouver l'erreur : à supprimer ===\n",
    "    print('pattern 1 : avant la boucle')\n",
    "    #=== pattern pour trouver l'erreur : à supprimer ===\n",
    "    url = 'https://www.autoscout24.fr/lst-moto/?sort=price&desc=1&ustate=N%2CU&size=20&page=1&cy=F&priceto=99000&atype=B&fc=1&qry=&recommended_sorting_based_id=16f9d2b5-588d-9c51-e053-0100007f13ed&'\n",
    "    df_listing = pd.DataFrame({\"url\": [np.nan]})\n",
    "    url_row = {\"url\": \"\"}\n",
    "    nb_results = url_nb_results(url)\n",
    "    print(f'Il y a en tout {nb_results} annonces')\n",
    "    boucle = 0\n",
    "    try:\n",
    "        while nb_results>400:\n",
    "            boucle +=1\n",
    "            print('boucle numéro : ', boucle, '(de 20 url de 20 annonces)')\n",
    "            time.sleep(2)\n",
    "\n",
    "            #création du df url de listing de 20 pages : df_listing\n",
    "            for page in range(1, 21):\n",
    "                url_row['url'] = url.split('page=')[0] + 'page=' + str(page) + '&cy=' + url.split('&cy=')[1]\n",
    "                df_listing = df_listing.append(url_row, ignore_index=True)\n",
    "\n",
    "            #extraire le dernier prix du listing\n",
    "            time.sleep(1)\n",
    "            url_listing_last_price = df_listing.iloc[len(df_listing)-1,0]\n",
    "\n",
    "            #catch de la liste d'annonces light de la dernière url de la boucle en cours pour extraction dernier prix : \n",
    "            resp_single = requests.get(url_listing_last_price)\n",
    "            soup_ = BeautifulSoup(resp_single.content, 'html.parser')\n",
    "            announces_list = soup_.find_all(\"div\", class_=\"cl-list-element cl-list-element-gap\")\n",
    "            #=== print indicatif ===\n",
    "            print('la dernière des 20 pages contient ', len(announces_list), ' annonces')\n",
    "            #=== print indicatif ===\n",
    "            time.sleep(2)\n",
    "            price = extract_int(announces_list[19].find(\"div\", class_=\"cldt-summary-payment\").find(\"span\", class_=\"cldt-price sc-font-xl sc-font-bold\").text)\n",
    "\n",
    "            #création de la nouvelle url\n",
    "            url = url.split('priceto=')[0] + 'priceto=' + str(price) + '&atype=' + url.split('&atype=')[1]\n",
    "            url = url.split('&fc=')[0] + '&fc=' + str(boucle) + '&qry=&' + url.split('&qry=&')[1]\n",
    "\n",
    "            #=== ajout du corps de la fonction pour voir le pb : supprimer pour ne laisser que la fonction ===\n",
    "            nb_results = url_nb_results(url)\n",
    "            print('pour la prochaine boucle il reste : ', nb_results, 'résultats')\n",
    "            #=== ajout du corps de la fonction pour voir le pb : supprimer pour ne laisser que la fonction ===\n",
    "\n",
    "\n",
    "        #si on arrive à la dernière page dans laquelle il y a moins de 20 résultats :\n",
    "        else:\n",
    "            print('dernière boucle')\n",
    "            for page in range(1, url_nb_pages(url)):\n",
    "                url_row['url'] = url.split('page=')[0] + 'page=' + str(page) + '&cy=' + url.split('&cy=')[1]\n",
    "                df_listing = df_listing.append(url_row, ignore_index=True)\n",
    "\n",
    "        df_listing = df_listing.drop(0)\n",
    "        #============================================================================\n",
    "        \n",
    "        print('df_listing terminé : création des fichiers')\n",
    "        #url to scrap : stockage html de chaque listing d'annonces light dans des fichiers\n",
    "        for i in range(len(df_listing)-1):\n",
    "            url = df_listing.iloc[i,0]\n",
    "            response = requests.get(url)\n",
    "            file_name = source + \"-\" + str(i) + \"-\" + datetime_1\n",
    "\n",
    "            with open(f\"pages/{file_name}.html\", \"w\")  as file:\n",
    "                file.write(response.text)\n",
    "                file.close()\n",
    "\n",
    "        #End time\n",
    "        end_time = datetime.now()\n",
    "        td = end_time - start_time\n",
    "\n",
    "        #log update\n",
    "        log_import = pd.read_csv('log.csv')\n",
    "        log_new = pd.DataFrame({'source' : [source],\n",
    "                                'step' : ['scrap pages'],\n",
    "                                'status' : ['completed'],\n",
    "                                'time' : [datetime.now()],\n",
    "                                'details' : [f\"{td.seconds/60} minutes elapsed, {len(df_listing)} pages scrapped\"]})\n",
    "        log = log_import.append(log_new , ignore_index=True)\n",
    "        log.to_csv('log.csv', index=False)\n",
    "        print('over')\n",
    "    \n",
    "    except (ValueError, TypeError, NameError, KeyError, RuntimeWarning, IndexError) as err:\n",
    "    #log update\n",
    "      \n",
    "        log_import = pd.read_csv('log.csv')\n",
    "        log_new = pd.DataFrame({'source' : [source],\n",
    "                                'step' : ['scrap pages'],\n",
    "                                'status' : ['error'],\n",
    "                                'time' : [datetime.now()],\n",
    "                                'details' : [err]})\n",
    "        log = log_import.append(log_new , ignore_index=True)\n",
    "        log.to_csv('log.csv', index=False)\n",
    "    \n",
    "#     return df_listing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "appropriate-edition",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pattern 1 : avant la boucle\n",
      "Il y a en tout 3803 annonces\n",
      "boucle numéro :  1 (de 20 url de 20 annonces)\n",
      "la dernière des 20 pages contient  20  annonces\n",
      "pour la prochaine boucle il reste :  3415 résultats\n",
      "boucle numéro :  2 (de 20 url de 20 annonces)\n",
      "la dernière des 20 pages contient  20  annonces\n",
      "pour la prochaine boucle il reste :  3020 résultats\n",
      "boucle numéro :  3 (de 20 url de 20 annonces)\n",
      "la dernière des 20 pages contient  20  annonces\n",
      "pour la prochaine boucle il reste :  2645 résultats\n",
      "boucle numéro :  4 (de 20 url de 20 annonces)\n",
      "la dernière des 20 pages contient  20  annonces\n",
      "pour la prochaine boucle il reste :  2246 résultats\n",
      "boucle numéro :  5 (de 20 url de 20 annonces)\n",
      "la dernière des 20 pages contient  20  annonces\n",
      "pour la prochaine boucle il reste :  1873 résultats\n",
      "boucle numéro :  6 (de 20 url de 20 annonces)\n",
      "la dernière des 20 pages contient  20  annonces\n",
      "pour la prochaine boucle il reste :  1474 résultats\n",
      "boucle numéro :  7 (de 20 url de 20 annonces)\n",
      "la dernière des 20 pages contient  20  annonces\n",
      "pour la prochaine boucle il reste :  1080 résultats\n",
      "boucle numéro :  8 (de 20 url de 20 annonces)\n",
      "la dernière des 20 pages contient  20  annonces\n",
      "pour la prochaine boucle il reste :  731 résultats\n",
      "boucle numéro :  9 (de 20 url de 20 annonces)\n",
      "la dernière des 20 pages contient  20  annonces\n",
      "pour la prochaine boucle il reste :  354 résultats\n",
      "dernière boucle\n",
      "df_listing terminé : création des fichiers\n",
      "over\n"
     ]
    }
   ],
   "source": [
    "scrap_pages()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acceptable-benefit",
   "metadata": {},
   "source": [
    "Essai avec la dernière sauvegarde sur Notion : jeudi matin (reprise de la dernière version de mardi soir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "satisfactory-romance",
   "metadata": {},
   "outputs": [],
   "source": [
    "#le problème est au niveau de la request qui ne marche pas pour autoscout donc peut que ca soit bloqué par le site lui meme\n",
    "# mais ca ne marche pas pour les autres sites non plus"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
