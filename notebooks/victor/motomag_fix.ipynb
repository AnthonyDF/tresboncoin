{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66c681fb",
   "metadata": {},
   "source": [
    "# Motomag Fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e51f6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "PATH_TO_CSV = 'motomag/motomag.csv'\n",
    "PATH_TO_IMG_FOLDER = 'motomag/img'\n",
    "PATH_TO_PAGES_FOLDER = 'motomag/pages'\n",
    "PATH_TO_ANNONCES_FOLDER = 'motomag/annonces'\n",
    "\n",
    "\n",
    "def scraping_pages():\n",
    "    try:\n",
    "        # Start time\n",
    "        start_time = datetime.now()\n",
    "\n",
    "        # site to scrap\n",
    "        source = 'motomag'\n",
    "\n",
    "        # Scrap the first page to find the max page number and save this page as html\n",
    "        url = 'https://www.motomag.com/-cote-argus-moto-scooter-petites-annonces-.html?debut_listingAnnoncesMoto=0#pagination_listingAnnoncesMoto'\n",
    "        response = requests.get(url)\n",
    "\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        max_page = int(soup.find(rel=\"nofollow\").text)\n",
    "\n",
    "        for page_number in range(0, max_page+1):\n",
    "            print(\"motomag - page number:\", page_number)\n",
    "\n",
    "            # url to scrap\n",
    "            url = f'https://www.motomag.com/-cote-argus-moto-scooter-petites-annonces-.html?debut_listingAnnoncesMoto={page_number*30}#pagination_listingAnnoncesMoto'\n",
    "            bike_response = requests.get(url)\n",
    "            file_name = source + \"-\" + str(page_number) + \"-\" + start_time.strftime(\"%Y-%m-%d_%Hh%M\")\n",
    "\n",
    "            with open(PATH_TO_PAGES_FOLDER + f\"/{file_name}.html\", \"w\") as file:\n",
    "                file.write(bike_response.text)\n",
    "                file.close()\n",
    "\n",
    "            # time.sleep(random.randint(2, 3))\n",
    "\n",
    "        # End time\n",
    "        end_time = datetime.now()\n",
    "        td = end_time - start_time\n",
    "\n",
    "\n",
    "    except (ValueError, TypeError, NameError, KeyError, RuntimeWarning) as err:\n",
    "        print(\"error occured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ffafa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "scraping_pages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "40af43bc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import codecs\n",
    "from datetime import datetime\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "PATH_TO_CSV = 'motomag/motomag.csv'\n",
    "PATH_TO_IMG_FOLDER = 'motomag/img'\n",
    "PATH_TO_PAGES_FOLDER = 'motomag/pages'\n",
    "PATH_TO_ANNONCES_FOLDER = 'motomag/annonces'\n",
    "\n",
    "\n",
    "def scraping_to_dataframe(res, code, ref):\n",
    "    try:\n",
    "        source = 'motomag'\n",
    "        # Start time\n",
    "        start_time = datetime.now()\n",
    "\n",
    "        count = 0\n",
    "\n",
    "        reference = int(ref)\n",
    "        uniq_id = source + \"-\" + str(ref)\n",
    "        \n",
    "        # initialize list for dataframe\n",
    "        data = {'uniq_id': [],\n",
    "                'reference': [],\n",
    "                'brand': [],\n",
    "                'model': [],\n",
    "                'price': [],\n",
    "                'bike_year': [],\n",
    "                'bike_type': [],\n",
    "                'engine_size': [],\n",
    "                'mileage': [],\n",
    "                'source': [],\n",
    "                'scraped_date': [],\n",
    "                'url': [],\n",
    "                'age': [],\n",
    "                'code_annonce': [code]}\n",
    "        \n",
    "        model_soup = BeautifulSoup(res.content, \"html.parser\")\n",
    "\n",
    "        try:\n",
    "            count += 1\n",
    "            engine_size = int(model_soup.find('div', class_='f17').text.split('cm3')[0])\n",
    "            mileage = int(model_soup.find('div', class_='f17').text.split('cm3')[1].split(' km')[0])\n",
    "            bike_type = model_soup.select('h1.f14')[0].text\n",
    "            brand = model_soup.select('h1.fPtNaB:nth-child(2)')[0].text.split(' - ')[0].lower()\n",
    "            model = model_soup.select('h1.fPtNaB:nth-child(2)')[0].text.split(' - ')[1].lower()\n",
    "            price = model_soup.find(itemprop='prix').text.replace(' €', '')\n",
    "            year = model_soup.find('div', class_='f17').text.split('Année du modèle : ')[1]\n",
    "            bike_year = re.match(r'\\d{4}', year).group(0)\n",
    "            data['uniq_id'].append(uniq_id)\n",
    "            data['reference'].append(reference)\n",
    "            data['brand'].append(brand)\n",
    "            data['model'].append(model)\n",
    "            data['price'].append(price)\n",
    "            data['bike_year'].append(bike_year)\n",
    "            data['bike_type'].append(bike_type)\n",
    "            data['mileage'].append(mileage)\n",
    "            data['engine_size'].append(engine_size)\n",
    "            data[\"age\"].append(int(datetime.now().strftime(\"%Y\")) - int(bike_year))\n",
    "            data['url'].append(\"https://www.motomag.com/spip.php?page=pamoto&id_annonce=\" + str(reference))\n",
    "            data['source'].append(source)\n",
    "            data['scraped_date'].append(datetime.now())\n",
    "            df = pd.DataFrame(data)\n",
    "            # loading existing csv\n",
    "            data_hist = pd.read_csv(PATH_TO_CSV)\n",
    "            # merge dataframes\n",
    "            new_csv = pd.concat([data_hist, df], axis=0)\n",
    "            # export to csv\n",
    "            new_csv.to_csv(PATH_TO_CSV, index=False)\n",
    "            print(\"Motomag line added. New Shape: \" + str(new_csv.shape[0]))\n",
    "        except:\n",
    "            print(\"error encountered in file: \" + filename)\n",
    "\n",
    "        # End time\n",
    "        end_time = datetime.now()\n",
    "        td = end_time - start_time\n",
    "    \n",
    "\n",
    "        # remove duplicates\n",
    "        df = pd.read_csv(PATH_TO_CSV)\n",
    "        df.drop_duplicates(subset=['reference', 'price'], inplace=True)\n",
    "        df.to_csv(PATH_TO_CSV, index=False)\n",
    "\n",
    "    except (ValueError, TypeError, NameError, KeyError, RuntimeWarning) as err:\n",
    "        print(\"error occured scrapping to dataframe\")\n",
    "\n",
    "\n",
    "def scraping_annonces():\n",
    "    try:\n",
    "        # website source name\n",
    "        source = 'motomag'\n",
    "\n",
    "        # Start time\n",
    "        start_time = datetime.now()\n",
    "\n",
    "\n",
    "        # import previously scrapped\n",
    "        df_import = pd.read_csv(PATH_TO_CSV)\n",
    "        \n",
    "        count_annonce = 0\n",
    "\n",
    "        # iterate over html files in pages directory\n",
    "        for filename in [file for file in os.listdir(PATH_TO_PAGES_FOLDER) if file.endswith(\".html\")]:\n",
    "            f = codecs.open(PATH_TO_PAGES_FOLDER + f\"{'/'+filename}\", 'r')\n",
    "            soup = BeautifulSoup(f, \"html.parser\")\n",
    "            bike_soup = soup.find_all(class_=\"col-md-6 mt10\")\n",
    "\n",
    "            for bike in bike_soup:\n",
    "\n",
    "                bike_url = bike.find(\"a\").get('href')\n",
    "                reference = bike_url.split(\"=\")[-1]\n",
    "                uniq_id = source + '-' + bike_url.split(\"=\")[-1]\n",
    "                moto_carac = bike.select(\"div[class*='article-txt pa5']\")\n",
    "                code_annonce = moto_carac[0].text.replace(\"\\n\",\"\").replace(\" \", \"\")\n",
    "                price = bike.find('span').text.replace(' €', '')\n",
    "                \n",
    "                # test if the bike with the same price is already in the databse\n",
    "                if code_annonce not in list(df_import[\"code_annonce\"]):\n",
    "                    count_annonce += 1\n",
    "\n",
    "                    response = requests.get(bike_url)\n",
    "                    scraping_to_dataframe(response, code_annonce, reference)\n",
    "                    \n",
    "                else:\n",
    "                    print(\"Announce already in dataset.\")\n",
    "\n",
    "            # delete html file\n",
    "            os.remove(PATH_TO_PAGES_FOLDER + \"/\" + filename)\n",
    "\n",
    "        # End time\n",
    "        end_time = datetime.now()\n",
    "        td = end_time - start_time\n",
    "\n",
    "\n",
    "    except (ValueError, TypeError, NameError, KeyError, RuntimeWarning) as err:\n",
    "        print(\"error occured scrapping announces\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eab3590",
   "metadata": {},
   "outputs": [],
   "source": [
    "scraping_annonces()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7322b937",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
